import openai
import json
import os
from sentence_transformers import SentenceTransformer
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from functools import lru_cache

# Ensure you have set the OPENAI_API_KEY in your environment variables
openai.api_key = os.getenv("OPENAI_API_KEY")

class RAGSystem:
    def __init__(self, knowledge_base):
        self.knowledge_base = knowledge_base
        self.model = SentenceTransformer('all-MiniLM-L6-v2')
        self.doc_embeddings = self.embed_knowledge_base()

    def embed_knowledge_base(self):
        docs = [doc["text"] for doc in self.knowledge_base]
        return self.model.encode(docs, convert_to_tensor=True)

    def normalize_query(self, query):
        """
        Normalize the query by converting it to lowercase and stripping whitespace.
        """
        return query.lower().strip()

    @lru_cache(maxsize=128)
    def retrieve(self, query, similarity_threshold=0.5, max_docs=5):
        # Normalize query for consistent caching
        normalized_query = self.normalize_query(query)
        print(f"Cache Access for retrieve: '{normalized_query}'")
        
        query_embedding = self.model.encode([normalized_query], convert_to_tensor=True)
        similarities = cosine_similarity(query_embedding, self.doc_embeddings)[0]
        
        similar_indices = np.where(similarities >= similarity_threshold)[0]
        sorted_indices = sorted(similar_indices, key=lambda i: similarities[i], reverse=True)
        top_indices = sorted_indices[:max_docs]

        retrieved_docs = [self.knowledge_base[i]["text"] for i in top_indices]

        if not retrieved_docs:
            max_index = np.argmax(similarities)
            retrieved_docs.append(self.knowledge_base[max_index]["text"])

        context = "\n\n".join(retrieved_docs)
        print("Retrieved Context:\n", context)

        return context

    @lru_cache(maxsize=128)
    def generate_response(self, query, context):
        # Normalize query for consistent caching
        normalized_query = self.normalize_query(query)
        print(f"Cache Access for generate_response: '{normalized_query}'")
        
        try:
            prompt = (
                "You are a helpful assistant specializing in cloud application development. "
                "Use the provided context to accurately answer the user's question.\n\n"
                "Context:\n" + context + "\n\n"
                "User Question: " + query + "\n\n"
                "Answer:"
            )

            response = openai.ChatCompletion.create(
                model="gpt-4-turbo",
                messages=[
                    {"role": "system", "content": "You are a helpful assistant."},
                    {"role": "system", "content": prompt},
                    {"role": "user", "content": normalized_query}
                ],
                temperature=0.5,
                max_tokens=300,
                top_p=1,
                frequency_penalty=0,
                presence_penalty=0
            )
            
            # Print the response generated by the model
            generated_response = response['choices'][0]['message']['content'].strip()
            
            # Indicate cache usage
            cache_info = self.generate_response.cache_info()
            if cache_info.misses > 0:
                print("No, this response is generated")
            else:
                print("Yes, this response is cached")
            
            return generated_response

        except openai.error.OpenAIError as e:
            print(f"Error generating response from OpenAI: {e}")
            return "An error occurred while generating the response."

    def answer_query(self, query):
        try:
            # Normalize query before use
            normalized_query = self.normalize_query(query)
            context = self.retrieve(normalized_query)
            response = self.generate_response(normalized_query, context)
            return response
        except Exception as e:
            print(f"Error in answer_query: {e}")
            return "An error occurred while generating the response."

    def cache_info(self):
        print("Retrieve cache info:", self.retrieve.cache_info())
        print("Generate response cache info:", self.generate_response.cache_info())

# Load knowledge base from a JSON file
with open('knowledge_base.json', 'r') as kb_file:
    knowledge_base = json.load(kb_file)

rag_system = RAGSystem(knowledge_base)

# Example usage
response = rag_system.answer_query("What is Defang?")
print("Response:", response)
rag_system.cache_info()  # Check cache statistics
