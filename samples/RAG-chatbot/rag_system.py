import openai
import json
import os
from sentence_transformers import SentenceTransformer
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from functools import lru_cache
import re

# Ensure you have set the OPENAI_API_KEY in your environment variables
openai.api_key = os.getenv("OPENAI_API_KEY")

class RAGSystem:
    def __init__(self, knowledge_base):
        self.knowledge_base = knowledge_base
        self.model = SentenceTransformer('all-MiniLM-L6-v2')
        self.doc_embeddings = self.embed_knowledge_base()

    def embed_knowledge_base(self):
        docs = [doc["text"] for doc in self.knowledge_base]
        return self.model.encode(docs, convert_to_tensor=True)

    def normalize_query(self, query):
        """
        Normalize the query by converting it to lowercase and stripping whitespace.
        """
        return query.lower().strip()

    def extract_keywords(self, query):
        """
        Extract keywords from the query using simple tokenization.
        """
        # Basic keyword extraction by splitting query into words
        return re.findall(r'\b\w+\b', query.lower())

    def fuzzy_match_about(self, query_keywords, about_text):
        """
        Check if any of the query keywords match the about text.
        """
        about_keywords = set(re.findall(r'\b\w+\b', about_text.lower()))
        matches = set(query_keywords) & about_keywords
        return len(matches)

    @lru_cache(maxsize=128)
    def retrieve(self, query, similarity_threshold=0.5, max_docs=5):
        # Normalize query for consistent caching
        normalized_query = self.normalize_query(query)
        print(f"Cache Access for retrieve: '{normalized_query}'")
        
        query_embedding = self.model.encode([normalized_query], convert_to_tensor=True)
        similarities = cosine_similarity(query_embedding, self.doc_embeddings)[0]
        
        query_keywords = self.extract_keywords(normalized_query)
        relevance_scores = []

        for i, doc in enumerate(self.knowledge_base):
            # Fuzzy match with the 'about' field
            about_score = self.fuzzy_match_about(query_keywords, doc.get('about', ''))
            total_score = similarities[i] + about_score
            relevance_scores.append((i, total_score))

        # Sort based on the combined score
        sorted_indices = sorted(relevance_scores, key=lambda x: x[1], reverse=True)
        top_indices = [i for i, score in sorted_indices[:max_docs] if score >= similarity_threshold]

        retrieved_docs = [self.knowledge_base[i]["text"] for i in top_indices]

        if not retrieved_docs:
            max_index = np.argmax(similarities)
            retrieved_docs.append(self.knowledge_base[max_index]["text"])

        context = "\n\n".join(retrieved_docs)
        print("Retrieved Context:\n", context)

        return context

    @lru_cache(maxsize=128)
    def generate_response(self, query, context):
        # Normalize query for consistent caching
        normalized_query = self.normalize_query(query)
        print(f"Cache Access for generate_response: '{normalized_query}'")
        
        try:
            prompt = (
                "You are a dedicated assistant designed to provide answers about Defang. "
                "When the user says 'you', 'your', or any pronoun, interpret it as referring to Defang with context of Defang also. "
                "If the user's question involves comparisons with or references to other services, you may use external knowledge. "
                "However, if the question is strictly about Defang, you must ignore all external knowledge and only utilize the given context. "
                "When generating the answer, please put the answer first and the justification later. "
                "Your objective is to remain strictly within the confines of the given context unless comparisons to other services are explicitly mentioned. "
                "\n\nContext:\n" + context + "\n\n"
                "User Question: " + query + "\n\n"
                "Answer:"
            )

            response = openai.ChatCompletion.create(
                model="gpt-4-turbo",
                messages=[
                    {"role": "system", "content": "You are a helpful assistant."},
                    {"role": "system", "content": prompt},
                    {"role": "user", "content": normalized_query}
                ],
                temperature=0.2,
                max_tokens=2048,
                top_p=1,
                frequency_penalty=0,
                presence_penalty=0
            )
            
            # Print the response generated by the model
            generated_response = response['choices'][0]['message']['content'].strip()
            
            # Indicate cache usage
            cache_info = self.generate_response.cache_info()
            if cache_info.misses > 0:
                print("No, this response is generated") 
            else:
                print("Yes, this response is cached")
            
            # Concatenate the context with the generated response
            final_response = f"**Context:**\n{context}\n\n**Response:**\n{generated_response}"
            
            return final_response
        except openai.error.OpenAIError as e:
            print(f"Error generating response from OpenAI: {e}")
            return "An error occurred while generating the response."

    def answer_query(self, query):
        try:
            # Normalize query before use
            normalized_query = self.normalize_query(query)
            context = self.retrieve(normalized_query)
            response = self.generate_response(normalized_query, context)
            return response
        except Exception as e:
            print(f"Error in answer_query: {e}")
            return "An error occurred while generating the response."

    def cache_info(self):
        print("Retrieve cache info:", self.retrieve.cache_info())
        print("Generate response cache info:", self.generate_response.cache_info())

# Load knowledge base from a JSON file
with open('knowledge_base.json', 'r') as kb_file:
    knowledge_base = json.load(kb_file)

rag_system = RAGSystem(knowledge_base)

# Example usage
response = rag_system.answer_query("What is Defang?")
print("Response:", response)
rag_system.cache_info()  # Check cache statistics
