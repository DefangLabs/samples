services:
  ollama:
    # uncomment to add your own domain
    # domainname: example.com
    # build:
    #   context: ./ollama
    #   dockerfile: Dockerfile
    #   shm_size: "16gb"
    image: ollama/ollama:latest
    x-defang-llm: 1
    ports:
      - target: 11434
        published: 11434
        mode: host
    # By Default we load llama3.2:1b because it can run efficiently on a CPU, but you can select
    # a different model by setting the MODEL environment variable. Check the [list](https://ollama.com/library)
    # for more models. For example, to load gemma:7b, set the MODEL environment variable to gemma:7b below.
    environment:
      - MODEL=meta.llama3-2-1b-instruct-v1:0

  ui:
    build:
      context: ui
      dockerfile: Dockerfile
    ports:
      - mode: ingress
        target: 3000
        published: 3000
    deploy:
      resources:
        reservations:
          memory: 256M
    healthcheck:
      # wget or curl required for healthchecks on services with a published port
      # this gets parsed by Defang and provided to the load balancers as well
      test: [ "CMD", "curl", "-s", "http://localhost:3000/" ]
    environment:
      - OLLAMA_ENDPOINT=http://ollama:11434
      # Make sure the MODEL is the same as the app service
      - MODEL=meta.llama3-2-1b-instruct-v1:0
