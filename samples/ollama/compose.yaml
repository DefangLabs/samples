services:
  ollama:
    build:
      context: ./ollama
      dockerfile: Dockerfile
      shm_size: 16gb
    ports:
    - target: 8000
      mode: host
    deploy:
      resources:
        reservations:
          cpus: '2.0'
          memory: 8192M
    environment:
    - LOAD_MODEL=llama3.2:1b
    healthcheck:
      test:
      - CMD
      - curl
      - -s
      - http://localhost:8000/
  ui:
    build:
      context: ui
      dockerfile: Dockerfile
    ports:
    - mode: ingress
      target: 3000
      published: 3000
    deploy:
      resources:
        reservations:
          memory: 256M
    healthcheck:
      test:
      - CMD
      - curl
      - -s
      - http://localhost:3000/
    environment:
    - OLLAMA_ENDPOINT=http://ollama:8000
    - LOAD_MODEL=llama3.2:1b
    depends_on:
    - ollama
name: ollama
