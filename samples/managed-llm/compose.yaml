name: managed-llm
services:
  app:
    build:
      context: ./app
      dockerfile: Dockerfile
    ports:
    - 8000:8000
    restart: always
    environment:
    - LLM_URL=http://llm/api/v1/
    - LLM_MODEL=default
    - OPENAI_API_KEY=FAKE_TOKEN
    healthcheck:
      test:
      - CMD
      - python3
      - -c
      - import sys, urllib.request; urllib.request.urlopen(sys.argv[1]).read()
      - http://localhost:8000/
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 5s
    depends_on:
    - llm
    networks:
    - default
    - private
  llm:
    x-defang-llm: true
    image: docker.io/defangio/openai-access-gateway:latest
    ports:
    - target: 80
      published: 80
      protocol: tcp
      mode: host
    networks:
    - private
    environment:
    - OPENAI_API_KEY=FAKE_TOKEN
networks:
  default: null
  private: null
